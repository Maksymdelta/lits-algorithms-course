\documentclass[12pt,a4paper]{report}

\input{../include/language.tex}
\input{../include/styles.tex}

\usepackage{amsmath}


% TODO: Remove this after merging the modules into a single master file.
\setcounter{chapter}{1}


\begin{document}

\chapter{Сортування}

% Insert table of contents.
\begingroup
\let\clearpage\relax
\tableofcontents
\endgroup



\section{Задача сортування}

Сортування --- це, зазвичай, найперша тема, яку вивчають студенти на дисциплінах «Основи програмування» чи «Алгоритми та структури даних».
Однак, при цьому кожна сучасна високорівнева мова програмування уже надає готові функції сортування у своїх системних бібліотеках.
Чому ж вартує вивчати алгоритми сортування та яку користь вони можуть принести?

\begin{itemize}
    \item Сортування --- одна з найпоширеніших задач, що виконується на комп’ютерах щоденно.
        Книга «Мистецтво програмування» Дональда Кнута наводить статистику одного дослідження 90-х років, яке показало, що чверть процесорного часу на великих комп'ютерах затрачалася на сортування даних.
        Відповідно, прискорення алгоритмів сортування суттєво впливає на продуктивність практично кожного комп’ютера в світі.
    \item Сортування --- це алгоритм, який лежить в основі багатьох складніших алгоритмів, або дозволяє їх відчутно прискорювати.
    \item Алгоритми сортування є хорошим джерелом алгоритмічних ідей.
        Так, парадигма «розділяй і володарюй» (``divide and conquer''), а також принцип рандомізації лежать в основі двох швидких і популярних алгоритмів сортування --- \emph{MergeSort} і \emph{QuickSort} відповідно.
\end{itemize}


\begin{minipage}{\linewidth}
\subsection*{Опис задачі}

В загальному випадку задача сортування описується так:

\begin{enumerate}
    \item Вхідні дані:
        \begin{itemize}
            \item Масив \(A\) довжиною \(N\) елементів.
            \item Функція |compare(a, b)|, яка може порівняти два елементи |a| та |b|, і повідомити, який з цих елементів менший.

                \emph{Примітка 1.} На практиці, для простоти коду ця функція може повертати |True|, якщо перший елемент менший за другий, і |False| в іншому випадку.

                \lstinputlisting{code/compare_function_natural.py}

                \emph{Примітка 2.} Часто ця функція не визначається в коді окремо, особливо коли ми сортуємо елементи в «натуральному» порядку --- наприклад, числа за абсолютною величиною, або ж стрічки за алфавітом.
                Проте її корисно виділити для випадку, коли критерій сортування складніший --- наприклад, сортування чисел за сумою цифр, стрічок за довжиною тощо.

                \lstinputlisting{code/compare_function_complex.py}
        \end{itemize}
    \item Вихідні дані: вхідний масив, проте зі зміненим порядком елементів --- \(A'\). Елементи повинні бути розташованими таким чином, що будь-який елемент з меншим індексом є не більшим за будь-який елемент з більшим індексом.
\end{enumerate}

\emph{Примітка.} Всі приклади коду наведені мовою Python, і припускають, що масив індексується з нуля. Відповідно, в масиві з N елементів перший елемент буде мати індекс \(0\), останній --- \(N - 1\).
\end{minipage}



\section{Примітивні алгоритми сортування}

Найпростіші алгоритми сортування працюють повільно на великих об’ємах даних, проте вони мають дві переваги:

\begin{enumerate}
    \item Їх дуже легко реалізувати.
    \item Завдяки простій логіці та відсутності «підготовчого» коду вони швидко працюють на дуже малих об’ємах даних (наприклад, до 20 елементів) --- часто навіть швидше за такі досконаліші алгоритми, як MergeSort.
\end{enumerate}


\subsection{Сортування вибіркою (Selection Sort)}

\emph{Принцип роботи:} Знайти в масиві найменший елемент, обміняти його з елементом на позиції 0. З тих \(N-1\) елементів, що залишилися, знайти найменший, і обміняти його з позицією 1. Так повторити \(N - 1\) разів.

\begin{minipage}{\linewidth}
\lstinputlisting{code/selection_sort.py}
\end{minipage}

\emph{Складність:} Алгоритм має зовнішній та внутрішній цикл на \(O(N)\) ітерацій, тому загальна складність становить \(O(N) \cdot O(N) = O(N^2)\).

\emph{Пам’ять:} \(O(N)\) для зберігання масиву, \(O(1)\) допоміжної пам’яті.

\emph{Переваги:}
\begin{itemize}
    \item Передбачувана швидкодія --- алгоритм завжди працює за квадратичний час, незалежно від того, як впорядковані вхідні дані.
    \item Мала кількість операцій запису --- максимум \(N-1\). Елемент відразу опиняється на своєму остаточному місці.
\end{itemize}

\emph{Недоліки:} Перевага водночас є й недоліком --- якщо на вхід подати майже впорядкований масив, алгоритм проведе багато ітерацій даремно.


\subsection{Сортування вставками (Insertion Sort)}

\emph{Принцип роботи:} Для кожної позиції \(i\) від \(1\) до \(N - 1\) рухати \(i\)-й елемент ліворуч доти, доки зліва є більший за нього елемент.

\begin{minipage}{\linewidth}
\lstinputlisting{code/insertion_sort.py}
\end{minipage}

\emph{Складність:} Алгоритм має зовнішній та внутрішній цикл на \(O(N)\) ітерацій, тому загальна складність становить \(O(N) \cdot O(N) = O(N^2)\).

\emph{Пам’ять:} \(O(N)\) для зберігання масиву, \(O(1)\) допоміжної пам’яті.

\emph{Переваги:} Може працювати швидше, залежно від вхідних даних. Якщо вхідний масив буде уже впорядкований, алгоритм виконає \(\Omega(N)\) операцій -- лінійну кількість замість квадратичної. Оскільки часто в повсякденних задачах дані є частково посортованими, є сенс застосовувати саме цей алгоритм для малих масивів, на відміну від Selection Sort чи Bubble Sort.

\emph{Недоліки:} Якщо вхідний масив буде посортованим у зворотньому порядку, алгоритм виконає велику (проте, все ж, квадратичну) кількість операцій читання та запису.


\subsection{Сортування бульбашкою (Bubble Sort)}

\emph{Принцип роботи:} Просканувати масив від початку до кінця, перевіряючи сусідні пари елементів. Якщо пара не впорядкована --- поміняти елементи місцями. Повторювати операцію сканування доти, доки будуть знаходитися невпорядковані пари.

Таким чином, подібно до бульбашок в склянці, менші елементи будуть одночасно «спливати» до лівого краю масиву.

\begin{minipage}{\linewidth}
\lstinputlisting{code/bubble_sort.py}
\end{minipage}

\emph{Складність:} Алгоритм має зовнішній та внутрішній цикл на \(O(N)\) ітерацій, тому загальна складність становить \(O(N) \cdot O(N) = O(N^2)\).

\emph{Пам’ять:} \(O(N)\) для зберігання масиву, \(O(1)\) допоміжної пам’яті.

\emph{Переваги:} На відміну від Selection Sort, алгоритм майже не робить даремних ітерацій і зупиняється тоді, коли масив стає посортованим. Таким чином, в найкращому випадку Bubble Sort виконає лінійну кількість операцій.

\emph{Недоліки:} Аналогічні до Insertion Sort.


\pagebreak


\section{Швидкі алгоритми сортування}


\subsection{Сортування злиттям (Merge Sort)}

Merge Sort --- найвідоміший з алгоритмів, що реалізовують принцип «розділяй і володарюй» (Divide and Conquer):
\begin{enumerate}
    \item Розділити велику задачу на кілька однакових менших підзадач.
    \item Вирішити кожну підзадачу рекурсивно.
    \item Об’єднати рішення підзадач в рішення початкової, великої задачі.
\end{enumerate}

\emph{Принцип роботи:} Розділити вхідний масив навпіл, посортувати кожну половину (рекурсивно), об’єднати дві посортовані половини в одну.

\lstinputlisting{code/merge_sort.py}

\emph{Складність:} Складність функції |merge| --- \(O(n)\), де \(n\) --- розмір кожного з двох посортованих підмасивів.
На верхньому рівні рекурсії алгоритм вирішує \(2\) підзадачі по \(N/2\) елементів кожна, рівнем нижче --- \(4\) по \(N/4\), рівнем нижче --- \(8\) по \(N/8\) і т.д.
Таким чином, на кожному рівні виконується \(O(N)\) операцій.
Оскільки з кожним рівнем розмір підзадачі зменшується вдвічі, то всього рівнів є \(\log_2 N\).

Отже, загальна складність алгоритму: \(\log_2 N \cdot O(N) = O(N \log N)\).

\emph{Пам’ять:} \(O(N)\) для зберігання вхідного масиву, \(O(N)\) допоміжної пам’яті для злиття двох посортованих підмасивів.

\emph{Переваги:} Передбачувана швидкодія. При будь-яких вхідних даних алгоритм гарантовано працюватиме за \(O(N \log N)\).

\emph{Недоліки:}
\begin{itemize}
    \item Для об’єднання підмасивів потрібно виділяти допоміжний масив --- отже, неможливо посортувати масив «на місці».
    \item Рекурсивні виклики можуть дещо зашкодити швидкодії. Однак, існує модифікація цього алгоритму --- Bottom-Up Merge Sort, яка працює без рекурсії.
\end{itemize}


\subsection{Швидке сортування (Quicksort)}

QuickSort --- елегантний і швидкий алгоритм, який, на відміну від Merge Sort, не використовує додаткової пам’яті. Він застосовує елемент рандомізації, і при правильній реалізації, дозволяє очікувати швидкодію, близьку до \(O(N \log N)\).

\emph{Принцип роботи:} Обрати довільний елемент масиву (|pivot|). Розташувати решту елементів так, щоб в лівій частині були елементи, менші за |pivot|, а в правій --- більші за |pivot|. Після цього посортувати ліву й праву частину рекурсивно.

\begin{minipage}{\linewidth}
\lstinputlisting{code/quick_sort_without_shuffling.py}
\end{minipage}

\emph{Складність:} Дана реалізація в більшості випадків працюватиме швидко, проте у неї є один суттєвий недолік.
Якщо в якості |pivot| буде обраний найменший чи найбільший елемент, ми виконаємо \(O(N)\) операцій на формування лівої та правої частини, проте одна з цих двох частин буде містити 0 елементів.
Таким чином, підзадача скоротиться тільки на 1 елемент --- початково обраний |pivot|.
Якщо на вхід прийде уже впорядкований масив (в прямому чи зворотньому порядку), тоді нам «не щаститиме» на кожному кроці, і складність алгоритму вироджується до \(N \cdot (N - 1) \cdot (N - 2) \cdot ... \cdot 1 = O(N^2)\).

Існує спосіб запобігти цьому. Перед тим, як сортувати вхідний масив, ми \emph{перемішуємо його елементи у випадковому порядку}. Існує алгоритм Fisher-Yates-Durstenfeld Shuffle (також відомий як Knuth Shuffle --- тасування Кнута), який здатен перетасувати масив за \(O(N)\) часу та \(O(1)\) пам’яті.

Тепер наша реалізація виглядатиме таким чином:

\begin{minipage}{\linewidth}
\lstinputlisting{code/quick_sort_with_shuffling.py}
\end{minipage}

\emph{Складність:} В середньому \(O(N \log N)\). Оскільки елемент |pivot| ми обираємо довільним чином, доведення базується на теорії ймовірностей, і є досить нетривіальним. Його можна прочитати \href{http://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D1%8B%D1%81%D1%82%D1%80%D0%B0%D1%8F_%D1%81%D0%BE%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0#.D0.A1.D1.80.D0.B5.D0.B4.D0.BD.D0.B5.D0.B5_.D0.B2.D1.80.D0.B5.D0.BC.D1.8F_.D1.80.D0.B0.D0.B1.D0.BE.D1.82.D1.8B}{за цим посиланням}.


\emph{Пам’ять:} \(O(N)\) для зберігання вхідного масиву, \(O(1)\) допоміжної пам’яті.

\emph{Переваги:} Не потребує додаткової пам’яті. Сортування відбувається дуже швидко на типових наборах даних.

\emph{Недоліки:} Немає строгої гарантії швидкості роботи --- теоретична межа \(O(N^2)\) все одно існує. Крім того, рекурсивні виклики можуть дещо зашкодити швидкодії.


\subsection{Інші застосування парадигми ``Divide and Conquer''}

\subsubsection*{Цілочисельне множення Карацуби}

У попередньому модулі ми розглядали шкільний алгоритм множення у стовпчик, який працював за час \(O(n^2)\), де \(n\) --- кількість розрядів чисел-множників.

Тепер подивимося, чи можемо ми застосувати наш новий прийом --- ``розділяй і володарюй'', щоб вирішити цю задачу ефективніше.

Візьмемо ті ж значення \(A\) та \(B\), що й були раніше: 5678 та 1234. Спробуємо розділити ці числа навпіл порозрядно. Таким чином \(A\) ділиться на 56 та 78, а \(B\) --- на 12 та 34.

Позначимо ці половини 56, 78, 12, 34 як \(a, b, c, d\) відповідно. Тепер ми можемо записати наш вираз множення так:

\begin{align*}
A \cdot B
& = (5600 + 78)(1200 + 34)
= (100a + b)(100c + d) \\
& = (10 ^ {\frac{n}{2}} a + b)(10 ^ {\frac{n}{2}} c + d) = \\
& = 10 ^ n a c + 10 ^ {\frac{n}{2}}(ad + bc) + bd
\end{align*}

Ми звели одну операцію множення \(AB\) до чотирьох операцій множення: \(ac, ad, bc, bd\). Кожна з цих чотирьох менших операцій множення працюватиме за час, пропорційний до \((\frac{n}{2})^2 = \frac{n^2}{4} \), отже виглядає, що ми зовсім не зекономили часу на такому перетворенні.

Однак існує прийом, придуманий Гаусом: нам не обов’язково знати окремі значення \(ad\) та \(bc\), а лише їх суму \(ad + bc\). І ми можемо її дізнатися таким способом:

\begin{enumerate}
    \item Обчислити добуток \(ac\).
    \item Обчислити добуток \(bd\).
    \item Обчислити добуток \((a + b)(c + d) = ac + ad + bc + bd\).
\end{enumerate}

Тепер ми можемо обчислити \(ad + bc\) як \((3) - (1) - (2)\)!
Отже, ми зекономили одну операцію множення --- тепер замість чотирьох операцій нам потрібно виконати лише три. І це дозволить нам скоротити час роботи алгоритму множення з \(O(n ^ 2)\) до \(O(n ^ {\approx 1.585})\). Звідки береться цей дивний показник степеня, ми дізнаємося з наступного розділу.


\subsection{Оцінка складності ``Divide and Conquer''. Master Theorem}



\section{Спеціалізовані алгоритми сортування}

Усі алгоритми, які ми розглядали до цього часу, базувалися на порівняннях. Ми не знали, які дані ми сортуємо, натомість у нас була лише функція |compare(a, b)|.

Серед розглянутих алгоритмів були як повільніші, що працюють за \(O(N^2)\), так і швидші, які працюють за \(O(N \log N)\).

Чи можемо ми сортувати масиви швидше? На жаль, ні --- \href{http://www.bowdoin.edu/~ltoma/teaching/cs231/fall07/Lectures/sortLB.pdf}{доведено}, що алгоритми, які базуються на порівняннях, не можуть мати кращу асимптотичну складність для найгіршого випадку, ніж \(O(N \log N)\).

Проте алгоритми не обов’язково повинні базуватися на порівняннях. Якщо ми знаємо певні додаткові деталі про тип чи діапазон даних, які ми сортуємо, ми можемо придумати швидший алгоритм спеціального призначення, який працюватиме лише з такими даними.

На сьогоднішній день найвідомішими з таких алгоритмів є Counting Sort, Radix Sort та Bucket Sort. Зокрема, Counting Sort здатен працювати за лінійний час --- \(O(N)\) --- типово, для випадку, коли ми сортуємо масиви цілих чисел, кожне з яких має невеликий діапазон (наприклад, 1 млн елементів від 0 до 255).



\section*{Ресурси для поглибленого вивчення}
\begin{enumerate}
    \item Візуалізація алгоритмів сортування. \href{http://www.sorting-algorithms.com/}{http://www.sorting-algorithms.com/}
    \item Robert Sedgewick, Kevin Wayne. Algorithms, 4\textsuperscript{th} edition: {\itshape Chapter 2: Sorting}.
    \item Cormen, Leierson, Rivest, Stein. Introduction to Algorithms, 3\textsuperscript{rd} edition: {\itshape Chapter 8: Sorting in Linear Time}.
\end{enumerate}

\end{document}
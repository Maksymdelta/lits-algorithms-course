\documentclass[12pt,a4paper]{report}

\usepackage{amsmath}
\usepackage{tikz}

\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes.misc}

\input{../include/language.tex}
\input{../include/styles.tex}


% TODO: Remove this after merging the modules into a single master file.
\setcounter{chapter}{1}

\tikzset{
    basic/.style  = {draw, text width=20pt, circle, align=center}
}

\begin{document}

\chapter{Сортування}

% Insert table of contents.
\begingroup
\let\clearpage\relax
\tableofcontents
\endgroup

\pagebreak



\section{Задача сортування}

Сортування --- це, зазвичай, найперша тема, яку вивчають студенти на дисциплінах «Основи програмування» чи «Алгоритми та структури даних».
Однак, при цьому кожна сучасна високорівнева мова програмування уже надає готові функції сортування у своїх системних бібліотеках.
Чому ж вартує вивчати алгоритми сортування та яку користь вони можуть принести?

\begin{itemize}
    \item Сортування --- одна з найпоширеніших задач, що виконується на комп’ютерах щоденно.
        Книга «Мистецтво програмування» Дональда Кнута наводить статистику одного дослідження 90-х років, яке показало, що чверть процесорного часу на великих комп'ютерах затрачалася на сортування даних.
        Відповідно, прискорення алгоритмів сортування суттєво впливає на продуктивність практично кожного комп’ютера в світі.
    \item Сортування --- це алгоритм, який лежить в основі багатьох складніших алгоритмів, або дозволяє їх відчутно прискорювати.
    \item Алгоритми сортування є хорошим джерелом алгоритмічних ідей.
        Так, парадигма «розділяй і володарюй» (``divide and conquer''), а також принцип рандомізації лежать в основі двох швидких і популярних алгоритмів сортування --- \emph{MergeSort} і \emph{QuickSort} відповідно.
\end{itemize}


\begin{minipage}{\linewidth}
\subsection*{Опис задачі}

В загальному випадку задача сортування описується так:

\begin{enumerate}
    \item Вхідні дані:
        \begin{itemize}
            \item Масив \(A\) довжиною \(N\) елементів.
            \item Функція |compare(a, b)|, яка може порівняти два елементи |a| та |b|, і повідомити, який з цих елементів менший.

                \emph{Примітка 1.} На практиці, для простоти коду ця функція може повертати |True|, якщо перший елемент менший за другий, і |False| в іншому випадку.

                \lstinputlisting{code/compare_function_natural.py}

                \emph{Примітка 2.} Часто ця функція не визначається в коді окремо, особливо коли ми сортуємо елементи в «натуральному» порядку --- наприклад, числа за абсолютною величиною, або ж стрічки за алфавітом.
                Проте її корисно виділити для випадку, коли критерій сортування складніший --- наприклад, сортування чисел за сумою цифр, стрічок за довжиною тощо.

                \lstinputlisting{code/compare_function_complex.py}
        \end{itemize}
    \item Вихідні дані: вхідний масив, проте зі зміненим порядком елементів --- \(A'\). Елементи повинні бути розташованими таким чином, що будь-який елемент з меншим індексом є не більшим за будь-який елемент з більшим індексом.
\end{enumerate}

\emph{Примітка.} Всі приклади коду наведені мовою Python, і припускають, що масив індексується з нуля. Відповідно, в масиві з N елементів перший елемент буде мати індекс \(0\), останній --- \(N - 1\).
\end{minipage}



\section{Примітивні алгоритми сортування}

Найпростіші алгоритми сортування працюють повільно на великих об’ємах даних, проте вони мають дві переваги:

\begin{enumerate}
    \item Їх дуже легко реалізувати.
    \item Завдяки простій логіці та відсутності «підготовчого» коду вони швидко працюють на дуже малих об’ємах даних (наприклад, до 20 елементів) --- часто навіть швидше за такі досконаліші алгоритми, як MergeSort.
\end{enumerate}


\subsection{Сортування вибіркою (Selection Sort)}

\emph{Принцип роботи:} Знайти в масиві найменший елемент, обміняти його з елементом на позиції 0. З тих \(N-1\) елементів, що залишилися, знайти найменший, і обміняти його з позицією 1. Так повторити \(N - 1\) разів.

\begin{minipage}{\linewidth}
\lstinputlisting{code/selection_sort.py}
\end{minipage}

\emph{Складність:} Алгоритм має зовнішній та внутрішній цикл на \(O(N)\) ітерацій, тому загальна складність становить \(O(N) \cdot O(N) = O(N^2)\).

\emph{Пам’ять:} \(O(N)\) для зберігання масиву, \(O(1)\) допоміжної пам’яті.

\emph{Переваги:}
\begin{itemize}
    \item Передбачувана швидкодія --- алгоритм завжди працює за квадратичний час, незалежно від того, як впорядковані вхідні дані.
    \item Мала кількість операцій запису --- максимум \(N-1\). Елемент відразу опиняється на своєму остаточному місці.
\end{itemize}

\emph{Недоліки:} Перевага водночас є й недоліком --- якщо на вхід подати майже впорядкований масив, алгоритм проведе багато ітерацій даремно.


\subsection{Сортування вставками (Insertion Sort)}

\emph{Принцип роботи:} Для кожної позиції \(i\) від \(1\) до \(N - 1\) рухати \(i\)-й елемент ліворуч доти, доки зліва є більший за нього елемент.

\begin{minipage}{\linewidth}
\lstinputlisting{code/insertion_sort.py}
\end{minipage}

\emph{Складність:} Алгоритм має зовнішній та внутрішній цикл на \(O(N)\) ітерацій, тому загальна складність становить \(O(N) \cdot O(N) = O(N^2)\).

\emph{Пам’ять:} \(O(N)\) для зберігання масиву, \(O(1)\) допоміжної пам’яті.

\emph{Переваги:} Може працювати швидше, залежно від вхідних даних. Якщо вхідний масив буде уже впорядкований, алгоритм виконає \(\Omega(N)\) операцій -- лінійну кількість замість квадратичної. Оскільки часто в повсякденних задачах дані є частково посортованими, є сенс застосовувати саме цей алгоритм для малих масивів, на відміну від Selection Sort чи Bubble Sort.

\emph{Недоліки:} Якщо вхідний масив буде посортованим у зворотньому порядку, алгоритм виконає велику (проте, все ж, квадратичну) кількість операцій читання та запису.


\subsection{Сортування бульбашкою (Bubble Sort)}

\emph{Принцип роботи:} Просканувати масив від початку до кінця, перевіряючи сусідні пари елементів. Якщо пара не впорядкована --- поміняти елементи місцями. Повторювати операцію сканування доти, доки будуть знаходитися невпорядковані пари.

Таким чином, подібно до бульбашок в склянці, менші елементи будуть одночасно «спливати» до лівого краю масиву.

\begin{minipage}{\linewidth}
\lstinputlisting{code/bubble_sort.py}
\end{minipage}

\emph{Складність:} Алгоритм має зовнішній та внутрішній цикл на \(O(N)\) ітерацій, тому загальна складність становить \(O(N) \cdot O(N) = O(N^2)\).

\emph{Пам’ять:} \(O(N)\) для зберігання масиву, \(O(1)\) допоміжної пам’яті.

\emph{Переваги:} На відміну від Selection Sort, алгоритм майже не робить даремних ітерацій і зупиняється тоді, коли масив стає посортованим. Таким чином, в найкращому випадку Bubble Sort виконає лінійну кількість операцій.

\emph{Недоліки:} Аналогічні до Insertion Sort.

\pagebreak



\section{Швидкі алгоритми сортування}


\subsection{Сортування злиттям (Merge Sort)}

Merge Sort --- найвідоміший з алгоритмів, що реалізовують принцип «розділяй і володарюй» (Divide and Conquer):
\begin{enumerate}
    \item Розділити велику задачу на кілька однакових менших підзадач.
    \item Вирішити кожну підзадачу рекурсивно.
    \item Об’єднати рішення підзадач в рішення початкової, великої задачі.
\end{enumerate}

\emph{Принцип роботи:} Розділити вхідний масив навпіл, посортувати кожну половину (рекурсивно), об’єднати дві посортовані половини в одну.

\lstinputlisting{code/merge_sort.py}

\emph{Складність:} Складність функції |merge| --- \(O(n)\), де \(n\) --- розмір кожного з двох посортованих підмасивів.
На верхньому рівні рекурсії алгоритм вирішує \(2\) підзадачі по \(N/2\) елементів кожна, рівнем нижче --- \(4\) по \(N/4\), рівнем нижче --- \(8\) по \(N/8\) і т.д.
Таким чином, на кожному рівні виконується \(O(N)\) операцій.
Оскільки з кожним рівнем розмір підзадачі зменшується вдвічі, то всього рівнів є \(\log_2 N\).

Отже, загальна складність алгоритму: \(\log_2 N \cdot O(N) = O(N \log N)\).

\emph{Пам’ять:} \(O(N)\) для зберігання вхідного масиву, \(O(N)\) допоміжної пам’яті для злиття двох посортованих підмасивів.

\emph{Переваги:} Передбачувана швидкодія. При будь-яких вхідних даних алгоритм гарантовано працюватиме за \(O(N \log N)\).

\emph{Недоліки:}
\begin{itemize}
    \item Для об’єднання підмасивів потрібно виділяти допоміжний масив --- отже, неможливо посортувати масив «на місці».
    \item Рекурсивні виклики можуть дещо зашкодити швидкодії. Однак, існує модифікація цього алгоритму --- Bottom-Up Merge Sort, яка працює без рекурсії.
\end{itemize}


\subsection{Швидке сортування (Quicksort)}

QuickSort --- елегантний і швидкий алгоритм, який, на відміну від Merge Sort, не використовує додаткової пам’яті. Він застосовує елемент рандомізації, і при правильній реалізації, дозволяє очікувати швидкодію, близьку до \(O(N \log N)\).

\emph{Принцип роботи:} Обрати довільний елемент масиву (|pivot|). Розташувати решту елементів так, щоб в лівій частині були елементи, менші за |pivot|, а в правій --- більші за |pivot|. Після цього посортувати ліву й праву частину рекурсивно.

\begin{minipage}{\linewidth}
\lstinputlisting{code/quick_sort_without_shuffling.py}
\end{minipage}

\emph{Складність:} Дана реалізація в більшості випадків працюватиме швидко, проте у неї є один суттєвий недолік.
Якщо в якості |pivot| буде обраний найменший чи найбільший елемент, ми виконаємо \(O(N)\) операцій на формування лівої та правої частини, проте одна з цих двох частин буде містити 0 елементів.
Таким чином, підзадача скоротиться тільки на 1 елемент --- початково обраний |pivot|.
Якщо на вхід прийде уже впорядкований масив (в прямому чи зворотньому порядку), тоді нам «не щаститиме» на кожному кроці, і складність алгоритму вироджується до \(N \cdot (N - 1) \cdot (N - 2) \cdot ... \cdot 1 = O(N^2)\).

Існує спосіб запобігти цьому. Перед тим, як сортувати вхідний масив, ми \emph{перемішуємо його елементи у випадковому порядку}. Існує алгоритм Fisher-Yates-Durstenfeld Shuffle (також відомий як Knuth Shuffle --- тасування Кнута), який здатен перетасувати масив за \(O(N)\) часу та \(O(1)\) пам’яті.

Тепер наша реалізація виглядатиме таким чином:

\begin{minipage}{\linewidth}
\lstinputlisting{code/quick_sort_with_shuffling.py}
\end{minipage}

\emph{Складність:} В середньому \(O(N \log N)\). Оскільки елемент |pivot| ми обираємо довільним чином, доведення базується на теорії ймовірностей, і є досить нетривіальним. Його можна прочитати \href{http://neerc.ifmo.ru/wiki/index.php?title=%D0%91%D1%8B%D1%81%D1%82%D1%80%D0%B0%D1%8F_%D1%81%D0%BE%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%BA%D0%B0#.D0.A1.D1.80.D0.B5.D0.B4.D0.BD.D0.B5.D0.B5_.D0.B2.D1.80.D0.B5.D0.BC.D1.8F_.D1.80.D0.B0.D0.B1.D0.BE.D1.82.D1.8B}{за цим посиланням}.


\emph{Пам’ять:} \(O(N)\) для зберігання вхідного масиву, \(O(1)\) допоміжної пам’яті.

\emph{Переваги:} Не потребує додаткової пам’яті. Сортування відбувається дуже швидко на типових наборах даних.

\emph{Недоліки:} Немає строгої гарантії швидкості роботи --- теоретична межа \(O(N^2)\) все одно існує. Крім того, рекурсивні виклики можуть дещо зашкодити швидкодії.


\subsection{Спеціалізовані алгоритми сортування}

Усі алгоритми, які ми розглядали до цього часу, базувалися на порівняннях. Ми не знали, які дані ми сортуємо, натомість у нас була лише функція |compare(a, b)|.

Серед розглянутих алгоритмів були як повільніші, що працюють за \(O(N^2)\), так і швидші, які працюють за \(O(N \log N)\).

Чи можемо ми сортувати масиви швидше? На жаль, ні --- \href{http://www.bowdoin.edu/~ltoma/teaching/cs231/fall07/Lectures/sortLB.pdf}{доведено}, що алгоритми, які базуються на порівняннях, не можуть мати кращу асимптотичну складність для найгіршого випадку, ніж \(O(N \log N)\).

Проте алгоритми не обов’язково повинні базуватися на порівняннях. Якщо ми знаємо певні додаткові деталі про тип чи діапазон даних, які ми сортуємо, ми можемо придумати швидший алгоритм спеціального призначення, який працюватиме лише з такими даними.

На сьогоднішній день найвідомішими з таких алгоритмів є Counting Sort, Radix Sort та Bucket Sort. Зокрема, Counting Sort здатен працювати за лінійний час --- \(O(N)\) --- типово, для випадку, коли ми сортуємо масиви цілих чисел, кожне з яких має невеликий діапазон (наприклад, 1 млн елементів від 0 до 255).



\section{Інші споріднені задачі}

\subsection{Інші застосування парадигми ``Divide and Conquer''}

\subsubsection*{Цілочисельне множення Карацуби}

У попередньому модулі ми розглядали шкільний алгоритм множення у стовпчик, який працював за час \(O(n^2)\), де \(n\) --- кількість розрядів чисел-множників.

Тепер подивимося, чи можемо ми застосувати наш новий прийом --- ``розділяй і володарюй'', щоб вирішити цю задачу ефективніше.

Візьмемо ті ж значення \(A\) та \(B\), що й були раніше: 5678 та 1234. Спробуємо розділити ці числа навпіл порозрядно. Таким чином \(A\) ділиться на 56 та 78, а \(B\) --- на 12 та 34.

Позначимо ці половини 56, 78, 12, 34 як \(a, b, c, d\) відповідно. Тепер ми можемо записати наш вираз множення так:

\begin{align*}
A \cdot B
& = (5600 + 78)(1200 + 34)
= (100a + b)(100c + d) \\
& = (10 ^ {\frac{n}{2}} a + b)(10 ^ {\frac{n}{2}} c + d) = \\
& = 10 ^ n a c + 10 ^ {\frac{n}{2}}(ad + bc) + bd
\end{align*}

Ми звели одну операцію множення \(AB\) до чотирьох операцій множення: \(ac, ad, bc, bd\). Кожна з цих чотирьох менших операцій множення працюватиме за час, пропорційний до \((\frac{n}{2})^2 = \frac{n^2}{4} \), отже виглядає, що ми зовсім не зекономили часу на такому перетворенні.

Однак існує прийом, винайдений Гаусом: нам не обов’язково знати окремі значення \(ad\) та \(bc\), а лише їх суму \(ad + bc\). І ми можемо її дізнатися таким способом:

\begin{enumerate}
    \item Обчислити добуток \(ac\).
    \item Обчислити добуток \(bd\).
    \item Обчислити добуток \((a + b)(c + d) = ac + ad + bc + bd\).
\end{enumerate}

Тепер ми можемо обчислити \(ad + bc\) як \((3) - (1) - (2)\)!
Отже, ми зекономили одну операцію множення --- тепер замість чотирьох операцій нам потрібно виконати лише три. І це дозволить нам скоротити час роботи алгоритму множення з \(O(n ^ 2)\) до \(O(n ^ {\approx 1.585})\). Звідки береться цей дивний показник степеня, ми дізнаємося з наступного розділу.


\subsection{Оцінка складності ``Divide and Conquer''. Master Theorem}

Пригадаймо, що основна ідея Divide and Conquer --- це розділити задачу на кілька однакових менших підзадач, розв’язати кожну з них та об’єднати результати. Таким чином, якщо описати час розв’язку більшої задачі як \(T(n)\), можна виразити \(T(n)\) через розв’язки менших задач таким чином:

\begin{center}
\begin{math}
T(n) \leq a T \left(\frac{n}{b}\right) + O(n ^ d)
\end{math}
\end{center}

\begin{itemize}
    \item \(a\) --- на скільки підзадач ми ділимо більшу задачу.
    \item \(b\) --- у скільки разів підзадачі менші, ніж більша задача.
    \item \(d\) --- степінь складності алгоритму об’єднання результатів.
\end{itemize}

Якщо ми можемо записати Divide and Conquer-алгоритм у вигляді виразу вище, де \(a, b, d\) не залежать від \(n\), тоді можна відразу визначити його асимптотичну складність за такою формулою (Master Theorem):

\begin{center}
\begin{math}
T(n) = 
    \begin{cases}
        O(n ^ d \log n),      & a = b ^ d \\
        O(n ^ d),             & a < b ^ d \\
        O(n ^ {\log_b a}),    & a > b ^ d
    \end{cases}
\end{math}
\end{center}

Спробуємо тепер проаналізувати складність MergeSort, використовуючи цей метод.

\begin{itemize}
    \item Оскільки MergeSort ділить більший масив на два менших, то \(a = 2\).
    \item Оскільки кожен підмасив удвічі менший за початковий, то \(b = 2\).
    \item Оскільки об’єднання результатів --- операція |merge| --- виконується за \(O(n) = O(n ^ 1)\), то \(d = 1\).
    \item \(a = 2, b ^ d = 2 ^ 1 = 2\), отже \(a = b ^ d\) (випадок 1 з формули). Тому складність MergeSort становить \(O(n ^ 1 \log n) = O(n \log n)\).
\end{itemize}

Тепер розглянемо алгоритм цілочисельного множення Карацуби:

\begin{itemize}
    \item Оскільки ми зводимо велику задачу множення до трьох менших задач множення (без трюку Гауса це було б чотири операції), то \(a = 3\).
    \item Оскільки кожне менше число має вдвічі менше розрядів, то \(b = 2\).
    \item Оскільки об’єднання результатів --- це операція додавання, яка виконується за \(O(n) = O(n ^ 1)\), то \(d = 1\).
    \item \(a = 3, b ^ d = 2 ^ 1 = 2\), отже \(a > b ^ d\) (випадок 3 з формули). Тому складність алгоритму Карацуби становить \(O(n ^ {\log_2 3}) \approx O(n ^ {1.585}) \).
\end{itemize}


\subsection{Статистики k-го порядку}

Інколи нам потрібно, маючи невпорядковані дані, знайти серед них \(k\)-й найменший чи найбільший елемент. Це називається статистикою \(k\)-го порядку (\(k\)-order statistic).

Для вирішення цієї задачі ми можемо посортувати наші дані, та вибрати \(k\)-й елемент серед посортованих. Проте виглядає, що ми вирішуємо складнішу задачу, ніж нам потрібно --- зрештою, нас цікавить лише один елемент.

Існує спосіб вирішити цю задачу швидше. Пригадаймо, як працює функція |partition| у QuickSort --- вона організовує елементи масиву так, що ліворуч опиняються елементи, менші за |pivot|, праворуч -- більші, та повертає позицію |p|, на якій опинився |pivot|. Таким чином, після операції |partition|, |pivot| є |p|-м найменшим елементом масиву.

Ми можемо опертися на функцію |partition| для вирішення нашої задачі. Якщо нам пощастило, і після її виклику вона повернула |p == k|, ми знайшли нашу статистику \(k\)-го порядку! Якщо ж ні, ми дивимося, чи |p > k|, чи |p < k|. У першому випадку ми рекурсивно викликаємо |partition| на правій частині, у другому --- на лівій.

Цей алгоритм називається QuickSelect (або Randomized Selection), і здатен працювати в середньому за \(O(N)\), що швидше, порівняно з повноцінним сортуванням за \(O(N \log N)\).


\subsection{Піраміди та черги з пріоритетами}

Споріднена із сортуванням задача --- реалізація черги з пріоритетами. Уявіть собі чергу в залізничну касу чи держустанову, в якій певні категорії людей мають право обслуговуватися в першу чергу, не залежно від того, в який час вони прийшли. Іншими словами, кожен відвідувач такої черги має числовий пріоритет \(P_i\), і ми мусимо завжди обслуговувати відвідувача з найвищим пріоритетом.

Наприклад, черга нижче повинна обслуговуватися в такому порядку: \(E, C, D, F, B, A\).

\begin{center}
    \footnotesize
    \begin{tikzpicture}
        \path (0.0,0) node[circle,draw] (a) {1} node at (0.0,-0.7) {A}
              (1.5,0) node[circle,draw] (b) {1} node at (1.5,-0.7) {B}
              (3.0,0) node[circle,draw] (c) {3} node at (3.0,-0.7) {C}
              (4.5,0) node[circle,draw] (d) {2} node at (4.5,-0.7) {D}
              (6.0,0) node[circle,draw] (e) {3} node at (6.0,-0.7) {E}
              (7.5,0) node[circle,draw] (f) {1} node at (7.5,-0.7) {F}
              (11.0,0) node[rectangle,draw] (sink) {вікно обслуговування};

        \path [draw,->,thick,-{Latex[length=2.5mm]}] (f) -- (sink);
        \draw (a) -- (b) -- (c) -- (d) -- (e) -- (f);
    \end{tikzpicture}
\end{center}

Нові клієнти приходять в процесі обслуговування, і черга повинна працювати коректно.

Операції, які повинна підтримувати черга:

\begin{enumerate}
    \item Додати новий елемент до черги, з вказаним пріоритетом (|Add|).
    \item Вилучити з черги елемент із максимальним пріоритетом (|ExtractMax|).
    \item Дізнатися, який елемент має найвищий пріоритет, не вилучаючи його (|Peek|).
\end{enumerate}

Спробуймо реалізувати чергу з пріоритетами на звичайному невпорядкованому масиві:

\begin{center}
    \begin{tikzpicture}
        \footnotesize
        \draw[step=1cm,gray,very thin] (0,0) grid (8,1);
        \draw node at (6.5,2.0) (insert-label) {місце вставки нового елемента};

        \normalsize
        \draw node at (0.5,0.5) {1};
        \draw node at (1.5,0.5) {3};
        \draw node at (2.5,0.5) {2};
        \draw node at (3.5,0.5) {3};
        \draw node at (4.5,0.5) {1};
        \draw node at (5.5,0.5) {1};
        \draw node at (6.5,0.5) (insert-cell) {};

        \draw [draw,->,thick,-{Latex[length=2.5mm]}] (insert-label) -- (insert-cell);
    \end{tikzpicture}
\end{center}

Тепер проаналізуємо, скільки часу займе кожна з потрібних нам операцій:

\begin{enumerate}
    \item |Add| --- оскільки ми завжди додаємо елементи в кінець, це займе \(O(1)\).
    \item |ExtractMax| --- оскільки елементи не впорядковані, нам потрібно щоразу знаходити максимум та зсовувати решту елементів ліворуч після вилучення, тому \(O(N)\).
    \item |Peek| --- оскільки елементи не впорядковані, нам потрібно щоразу знаходити максимум, тому \(O(N)\).
\end{enumerate}

Тепер, якщо ми вставимо та вилучимо \(N\) клієнтів, вилучення сумарно займе \(N \cdot O(N) = O(N^2)\). Можливо, буде краще, якщо ми завжди триматимемо цей масив впорядкованим за пріоритетом?

\begin{center}
    \begin{tikzpicture}
        \footnotesize
        \draw[step=1cm,gray,very thin] (0,0) grid (8,1);
        \draw node at (3,2.0) (insert-label) {місце вставки нового елемента з пріоритетом 2};

        \normalsize
        \draw node at (0.5,0.5) {3};
        \draw node at (1.5,0.5) {3};
        \draw node at (2.5,0.5) {2};
        \draw node at (3.5,0.5) {1};
        \draw node at (4.5,0.5) {1};
        \draw node at (5.5,0.5) {1};
        \draw node at (3,1) (insert-cell) {};

        \draw [draw,->,thick,-{Latex[length=2.5mm]}] (insert-label) -- (insert-cell);
    \end{tikzpicture}
\end{center}

Подивимося, чи став час кращим:

\begin{enumerate}
    \item |Add| --- оскільки нам завжди потрібно зберігати масив впорядкованим, може статися так, що нам потрібно буде вставити елемент із середнім пріоритетом, і тоді нам доведеться зсунути половину елементів праворуч. Тому час --- \(O(N)\).
    \item |ExtractMax| --- оскільки елементи уже впорядковані, нам потрібно просто вилучити найлівіший елемент. У випадку масиву це буде \(O(N)\), а якщо використаємо зв’язаний список --- \(O(1)\).
    \item |Peek| --- оскільки елементи уже впорядковані, ми просто повертаємо значення найлівішого елемента, тому \(O(1)\).
\end{enumerate}

Оскільки операція |Add| працює за \(O(N)\), вставка і вилучення \(N\) елементів все одно займе \(O(N^2)\) часу. Таким чином, сумарно ми не отримали виграшу від впорядкованості масиву.

На щастя, для черги з пріоритетами існує спосіб отримати час, кращий за квадратичний. Він полягає у застосуванні структури даних «піраміда» (heap). Піраміда підтримує саме ті три операції, які нам потрібні --- |Add|, |ExtractMax| та |Peek|.

Існує кілька способів реалізації піраміди. Ми розглянемо її найпростіший підвид --- бінарна піраміда (Binary Heap).

Binary Heap --- це бінарне дерево, яке має дві додаткові властивості:

\begin{enumerate}
    \item Всі рівні, крім останнього, мусять бути заповнені.
    \item Для будь-якого вузла повинна виконуватися властивість (heap property): значення вузла мусить бути не меншим, ніж значення кожного з його дітей.
\end{enumerate}

\begin{center}
    \begin{tikzpicture}
    \begin{scope}[every node/.style={basic}]
    \node                                                  (level1-1) {9};
    \node [below of = level1-1,xshift=-120pt,yshift=-20pt] (level2-1) {7};
    \node [below of = level1-1,xshift= 120pt,yshift=-20pt] (level2-2) {5};
    \node [below of = level2-1,xshift=-60pt,yshift=-20pt]  (level3-1) {4};
    \node [below of = level2-1,xshift= 60pt,yshift=-20pt]  (level3-2) {4};
    \node [below of = level2-2,xshift=-60pt,yshift=-20pt]  (level3-3) {5};
    \node [below of = level2-2,xshift= 60pt,yshift=-20pt]  (level3-4) {3};
    \node [below of = level3-1,xshift=-30pt,yshift=-20pt]  (level4-1) {1};
    \node [below of = level3-1,xshift= 30pt,yshift=-20pt]  (level4-2) {3};
    \node [below of = level3-2,xshift=-30pt,yshift=-20pt]  (level4-3) {2};
    \node [below of = level3-2,xshift= 30pt,yshift=-20pt]  (level4-4) {1};
    \node [below of = level3-3,xshift=-30pt,yshift=-20pt]  (level4-5) {4};

    \path [draw,thick] (level4-5) -- (level3-3);
    \path [draw,thick] (level4-4) -- (level3-2);
    \path [draw,thick] (level4-3) -- (level3-2);
    \path [draw,thick] (level4-2) -- (level3-1);
    \path [draw,thick] (level4-2) -- (level3-1);
    \path [draw,thick] (level4-1) -- (level3-1);
    \path [draw,thick] (level3-4) -- (level2-2);
    \path [draw,thick] (level3-3) -- (level2-2);
    \path [draw,thick] (level3-2) -- (level2-1);
    \path [draw,thick] (level3-1) -- (level2-1);
    \path [draw,thick] (level2-2) -- (level1-1);
    \path [draw,thick] (level2-1) -- (level1-1);
    \end{scope}

    \end{tikzpicture}
\end{center}

Операція вставки в Binary Heap відбувається так:
\begin{enumerate}
    \item Додаємо наш елемент на останній рівень (останній рівень завжди заповнюється зліва направо).
    \item Порівнюємо його з батьком. Якщо батько менший --- обмінюємо їх місцями.
    \item Повторюємо обміни доти, доки не досягнемо батька, який не менший за величиною, або поки не досягнемо кореня.
\end{enumerate}

Операція вилучення максимального елемента:
\begin{enumerate}
    \item Вилучаємо корінь, на його місце ставимо найправіший елемент з нижнього рівня.
    \item Якщо хтось із дітей нового кореня має більше значення, ніж корінь, обмінюємо корінь та кращу дитину місцями.
    \item «Топимо» наш елемент вниз до тих пір, поки обоє його дітей не будуть менші, або поки не досягнемо дна.
\end{enumerate}

Візуалізацію роботи Binary Heap можна подивитися на \href{http://visualgo.net/heap}{http://visualgo.net/heap}.

Проаналізуємо час виконання операцій:

\begin{enumerate}
    \item |Add| --- в результаті вставки новий вузол підніметься щонайбільше на \(h\) рівнів, де \(h\) --- висота дерева. Для заповненого бінарного дерева \(h = O(\log N)\), тому час вставки --- \(O(\log N)\).
    \item |ExtractMax| --- в результаті вилучення новий корінь опуститься щонайбільше на \(h\) рівнів, тому час вилучення --- \(O(\log N)\).
    \item |Peek| --- оскільки максимальний елемент завжди лежить в корені, ми просто повертаємо значення кореня, тому \(O(1)\).
\end{enumerate}

Тепер, якщо нам потрібно вставити в чергу \(N\) клієнтів та вилучити їх, сумарно це займе \(N \cdot O(\log N) + N \cdot O(\log N) = O(N \log N)\) операцій. Таким чином, ми зменшили найгірший час роботи з квадратичного до квазілінійного.

Можна було помітити, що фактично Heap працює таким чином: ми вкладаємо в нього невпорядковані елементи, а, вилучаючи, отримаємо уже впорядковані. Відповідно, ми можемо сортувати масиви за допомогою Heap! Достатньо просто внести всі елементи в Heap, а потім їх один за одним вилучити. Сумарно це займе \(O(N \log N)\) часу, отже, ми «винайшли» ще один асимптотично оптимальний алгоритм сортування, який має назву HeapSort.


\section*{Ресурси для поглибленого вивчення}
\begin{enumerate}
    \item Візуалізація алгоритмів сортування.

    \href{http://www.sorting-algorithms.com/}{http://www.sorting-algorithms.com/}
    
    \href{http://visualgo.net/}{http://visualgo.net/}

    \item Robert Sedgewick, Kevin Wayne. Algorithms, 4\textsuperscript{th} edition: {\itshape Chapter 2: Sorting}.
    \item Steven Skiena. The Algorithm Design Manual, 2\textsuperscript{nd} edition: {\itshape Chapter 4.10: Divide-and-Conquer}.
    \item Cormen, Leierson, Rivest, Stein. Introduction to Algorithms, 3\textsuperscript{rd} edition: {\itshape Chapter 8: Sorting in Linear Time}.
    \item Cormen, Leierson, Rivest, Stein. Introduction to Algorithms, 3\textsuperscript{rd} edition: {\itshape Chapter 9: Medians and Order Statistics}.
\end{enumerate}

\end{document}